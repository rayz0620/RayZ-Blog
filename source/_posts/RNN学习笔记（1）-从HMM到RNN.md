title: "RNN学习笔记（1）-从HMM到RNN"
date: 2015-05-14 11:40:19
tags: 
	- RNN
categories:
	- academic
description: 我们要建模前后相关的序列数据，我们要能推广到各种长度的序列，我们不要爆炸性增长的的网络参数，所以有了RNN。
---

## 前言
周一在实验室Seminar上讲了一下关于RNN的基本知识，就想着把讲的内容再整理整理写下来。RNN预计分成三篇讲博客，这一篇简要介绍RNN，它诞生的缘由，它的基本结构和特性。

讲Seminar的时候借用了香港中文大学[王小刚教授](http://www.ee.cuhk.edu.hk/~xgwang/)的Slides，文中部分图片也来自他的Slides，在此表示感谢。各位有兴趣的话可以自己看看，链接在此：[Slides](https://piazza.com/cuhk.edu.hk/spring2015/eleg5040/home)

## 序列数据
RNN是为了对序列数据进行建模而产生的。那么首先，什么是序列数据呢？其实常见的数据类型中，就有很多的序列数据。文本，是字母和词汇的序列；语音，是音节的序列；视频，是图像的序列；气象观测数据，股票交易数据等等，也都是序列数据。

从数学的角度来讲，先假设我们有这么一个数据样本的集合{% math X\{x_1, x_2, \ldots, x_N\} %}. 在一般的不考虑序列的机器学习问题里，我们假设每一个$x_i$都是相互**独立**的。不管他们是不是同分布，他们都是**独立**的, {% math P(x_i, x_j) = P(x_i) \cdot P(x_j) %}。

而序列数据正是将“样本之间相互独立”这一假设改成了“样本之间存在关联”。对样本之间的序列性最简单直接的一种定义是：样本间存在顺序关系，每个样本和它之前的样本存在关联。比如说，在文本中，一个词和它前面的词是有关联的；在气象数据中，一天的气温和前几天的气温是有关联的。一组观察数据定义为一个序列，从分布中可以观察出多个序列。

## Naive的序列模型
有了序列性的定义，我们可以开始做一个最简单，最纯粹，最Naive的序列模型了。对于一个序列{% math X\{x_1, x_2, \ldots, x_N\} %}，我们把整个序列的联合分布定义为

{% math_block %}
	P(X)=\prod_{i=1}^N{P(x_i|x_1,\ldots,x_{i-1})} 
{% endmath_block %}

也就是说，序列里的每一个元素都和排在它前面的**所有**元素**直接**相关。之所以说这个模型是Naive的，是因为它的复杂度会爆炸性增长。如果序列长度是$N$，{% math P(x_i|x_1,\ldots,x_{i-1}) %}的模型参数是{% math O(i)$ %}，连乘起来就是{% math O(N!)%}！模型参数随序列长度以阶乘速度增长，这是我们所承受不了的。

## 隐马尔科夫模型（HMM）
Naive模型中，模型复杂度随序列长度阶乘增长，复杂度太高。要怎么样减少模型的复杂度呢？Naive模型高复杂度的成因在于，序列中每个元素和它前面的**所有**元素相关。正是这个“**所有**”让它的复杂度爆炸性增长。那么，我们只要限制每个元素只和前面有限个元素关联就好了。一般我们认为，一元素和离它最越近的元素关联性越大。那么我们就定义每个元素只和离它最近的$k$个元素相关就好了，这就是$k$阶马尔科夫模型。现在整个序列的概率分布是这样的：
{% math_block %}
 P(X)=\prod_{i=1}^N{P(x_i|x_{i-1},\ldots,x_{i-(k-1)})} 
{% endmath_block %}

$k=1%时，称作1阶马尔科夫模型：
{% math_block %}
	P(X)=\prod_{i=1}^N{P(x_i|x_{i-1})} 
{% endmath_block %}
![First-Order Markov Model](/img/rnn1/first_order_markov.png)

只考虑观察值$X$的模型有时表现力不足，因此需要加入隐变量，将观察值建模成由隐变量所生成。隐变量的好处在于，它的数量可以比观察值多，取值范围可以比观察值更广，能够更好的表达有限的观察值背后的复杂分布。加入了隐变量$h$的马尔科夫模型称为隐马尔科夫模型。
{% math_block %}
	P(x_1, \ldots, x_T, h_1, \ldots, h_T, \theta) = P(h_1) \prod^T_{t=2}{P(h_t|h_{t-1})} \prod^T_{t=1}{(x_t|h_t)}
{% endmath_block %}
![Hidden Markov Model](/img/rnn1/hidden_markov.png)

隐马尔科夫模型实际上建模的是观察值$x$，隐变量$h$和模型参数$\theta$的联合分布。每个位置$t$的观察值由对应位置的隐变量{% math h_t %}生成。$h_t$由上一位置的隐变量{% math h_{t-1} %}生成。最初位置的隐变量{% math h_1 %}由一个先验分布{% math P(h_1) %}生成。

注意到，HMM的模型长度$T$是事先固定的，模型参数不共享。也就是说，{% math h_1 %} 到 {% math h_2 %}和{% math h_3 %}到{% math h_4 %}的变换参数是各自独立的。

## Recurrent Neural Network (RNN)
接下来终于要介绍真正的主角，Recurrent Neural Network(RNN)。RNN一般被翻译成“延时神经网络”，或者“递归神经网络”。个人认为，两种翻译从不同的角度描述了RNN的特性。“延时”偏重于模型的物理意义，而“递归”则反应了模型的数学特性。作为名称来说，前者的区分度较高，后者容易和Recursive Neural Network混淆（虽然两者确实存在关联性）。

![Recurrent Neural Network](/img/rnn1/rnn.png)
RNN是深度神经网络模型（DNN）的变种，为了序列建模而诞生。它继承了DNN的线性变换+非线性激活函数的模型结构，并且吸收了HMM模型的有限序列关联的思想。上图中，隐含层$h$有一条和自己连接的边，这条边就是实现“Recurrent”性质的关键。

为了更好描述RNN的性质，下文中把序列视作时间序列，每一个$t$称作一个“时刻”。$h$的自连接边实际上是和上一时刻的$h$相连。在每一个时刻$t$，{% math h_t %}的取值是当前时刻的输入{% math x_t %}，和上一时刻的隐含层值{% math h_{t-1} %}的一个函数：
{% math_block %}
	h_t = F_{\theta}(h_{t-1}, x_t)
{% endmath_block %}

将$h$层的自连接展开，就成为了上图右边的样子，看上去和HMM很像。两者最大的区别在于，RNN的参数是**跨时刻共享**的。也就是说，对任意时刻$t$，{% math h_{t-1} %}到{% math h_t %}以及{% math x_t %}到{% math h_t %}的网络参数都是相同的。共享参数的思想和和卷积神经网络（CNN）是相通的，CNN在二维数据的空间位置之间共享卷积核参数，而RNN则是在序列数据的时刻之间共享参数。

共享参数使得模型的复杂度大大减少。回顾一下之前提到的两个模型，Naive模型的复杂度是{% math O(T!) %}，而HMM的复杂度是{% math O(T) %}。RNN在引入了共享参数之后，复杂度和序列长度$T$不再有关联，从$T$的角度看，RNN的复杂度就是{% math O(1) %}。复杂度的减少让模型更好训练，不那么容易过拟合。

共享参数使RNN可以适应任意长度的序列，带来了更好的可推广性。现实中序列数据的长度往往是变化的。如果模型只能适应一种长度的序列，那么对于每一种可能的长度，我们要去单独训练一个模型。这样的训练需要大量的样本，因为我们要为每一种长度准备足够多的样本。这样的模型也不能学到不同长度序列之间的共性。例如，不同长度的句子都是同样的语言，共享同样的语法、语义和上下文相关规律。如果模型不能学到这些，将是很大的损失。RNN能够简单的应用于不同长度的序列，训练和预测都不需要特别的调整，这使得RNN作为一种序列模型具有优势。

## 结语
这一篇简要介绍了RNN诞生的缘由，它的基本特性。下一篇将仔细介绍如何训练和使用RNN。